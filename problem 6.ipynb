{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019bbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate A & b -> 300m 200n\n",
    "m,n=300,200\n",
    "A=np.random.normal(size=(m,n))\n",
    "b=np.random.normal(size=(m,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70f5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: (m, n), where m is the number of rows, and n is the number of columns.\n",
    "# note: column of first matrix = row of second matrix (200,300)@(300,)\n",
    "\n",
    "def objective(x, A, b):\n",
    "    return 0.5 * np.linalg.norm(A @ x - b)**2\n",
    "\n",
    "def grad(x, A, b):\n",
    "    return A.T @ (A @ x - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38532b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjugate gradient method converged after 50 iterations.\n",
      "Solution: [ 1.16374206e+125 -5.25204381e+124 -4.68195851e+123 -3.28405996e+124\n",
      " -2.01590428e+124  4.84136259e+124  9.81366427e+124 -8.21581168e+124\n",
      "  1.05960716e+125 -5.78692369e+124  2.93030963e+125 -6.70121037e+124\n",
      " -9.70532261e+124 -2.23303782e+125  1.47400754e+125 -1.85685813e+124\n",
      "  4.27237119e+124  6.06495299e+123 -1.58690507e+125  4.29532211e+124\n",
      "  9.74664926e+124  2.76346397e+124  1.69936391e+125  2.43709978e+125\n",
      "  2.84800002e+124  1.47918152e+125  1.24106067e+125  1.17792189e+125\n",
      "  1.53296821e+125 -6.89469830e+123  6.20236513e+124 -2.00051636e+125\n",
      "  4.73104934e+124 -1.51209865e+125 -6.35449114e+124 -2.00040645e+125\n",
      " -8.51599834e+124  1.97225737e+123  7.13823791e+124  4.30295907e+124\n",
      " -3.98502923e+124  1.14592333e+124  3.80126188e+124 -9.62223739e+124\n",
      " -1.25481788e+125  8.83191632e+124  3.39536472e+124  1.56877970e+123\n",
      "  1.06184177e+125  1.72651169e+124  1.23542597e+124  6.15550389e+124\n",
      " -3.72258668e+124 -1.26093495e+125 -6.81909539e+124 -1.67087425e+125\n",
      "  1.95425225e+125 -1.19559701e+125 -6.67473781e+124  9.27892541e+124\n",
      " -1.29453010e+125  4.36423697e+124  9.71842248e+123 -6.75704824e+124\n",
      " -2.21467703e+125  1.14197392e+125 -4.86276096e+124  1.55823359e+124\n",
      "  1.34750528e+125 -9.60582727e+124 -2.71533947e+124 -1.51463115e+125\n",
      " -1.57309315e+125 -2.51518663e+124 -1.05676231e+125  1.90636021e+125\n",
      "  6.10948269e+124 -1.38439507e+124  6.75472242e+124 -9.36540809e+122\n",
      " -1.14052426e+125  1.02858961e+125 -6.70377056e+124 -2.20175442e+125\n",
      "  5.64514601e+123 -6.82351782e+124  4.66701491e+124  8.13693684e+124\n",
      "  1.36679506e+125 -8.41450290e+124 -2.03930903e+125 -7.58199655e+124\n",
      "  3.33698618e+124 -7.12678610e+124  2.57020201e+124 -3.15667180e+123\n",
      " -2.05658780e+125 -9.17009657e+124 -3.38094229e+124  6.04113223e+123\n",
      "  2.43544710e+125  1.81716907e+125 -2.40678147e+125  1.88633848e+125\n",
      "  4.12226199e+124  1.50702977e+125  2.94721977e+125  5.47399453e+123\n",
      " -7.19810789e+124 -6.71915770e+124  9.56557019e+124 -2.52264964e+123\n",
      "  2.66191810e+124 -1.17357583e+125  1.69667333e+125 -4.43205934e+123\n",
      " -1.01991568e+123  1.56301705e+125  1.03790367e+125  7.07556018e+124\n",
      " -1.08167604e+124  1.00318494e+124  6.26977127e+124  3.07511387e+124\n",
      " -8.05993518e+124 -3.48962051e+124 -8.25384435e+124 -2.98410856e+124\n",
      " -1.01325220e+124 -1.70677648e+124 -4.45350111e+124 -6.09143660e+124\n",
      " -7.69616068e+124  3.86547920e+124  8.24279758e+122 -7.54118798e+122\n",
      " -1.23900308e+124  9.64011474e+124 -5.89405950e+123 -8.75452585e+124\n",
      " -8.34423251e+124 -7.93360105e+124  7.40282353e+124 -1.59662397e+125\n",
      " -4.46500591e+124 -1.20672246e+125  6.55930328e+124 -1.16728601e+125\n",
      " -4.24252750e+124 -1.12236419e+124  3.78599200e+124  2.78630039e+124\n",
      " -3.40473490e+124 -9.15629105e+123 -1.51537222e+125 -1.45291834e+125\n",
      " -7.66403406e+124  1.15140387e+125 -8.72347375e+124  1.52059237e+125\n",
      " -1.03578532e+124  1.65852915e+124  1.25782227e+125  1.18278143e+125\n",
      " -8.02023289e+124  8.84591522e+124  1.97998873e+125  1.08685609e+123\n",
      "  9.03453553e+124  8.24218996e+124  3.43112760e+124  6.92419845e+124\n",
      " -1.90997249e+125  5.56976729e+124 -6.06413812e+124 -1.17796999e+124\n",
      "  1.40766797e+125 -1.85646818e+125  2.21839782e+124 -1.54771480e+124\n",
      " -1.63299084e+124  3.66477156e+124 -2.29636091e+125  1.49550506e+125\n",
      "  1.95574739e+125  1.01175113e+125 -4.81813053e+124  1.16886696e+124\n",
      "  3.29258786e+124  3.18556944e+124 -5.17420105e+124  1.75799691e+125\n",
      " -2.05799566e+124 -3.62108111e+124 -2.74428636e+124  7.78452862e+124\n",
      " -1.09485570e+125 -2.76669654e+124 -1.93802051e+125 -1.00724502e+125]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set problem dimensions\n",
    "m = 300\n",
    "n = 200\n",
    "\n",
    "# Randomly generate A and b using Gaussian distribution\n",
    "A = np.random.normal(size=(m, n))\n",
    "b = np.random.normal(size=(m,))\n",
    "\n",
    "# Define the objective function and its gradient\n",
    "def objective(x):\n",
    "    return 0.5 * np.linalg.norm(A @ x - b)**2\n",
    "\n",
    "def grad(x):\n",
    "    return A.T @ (A @ x - b)\n",
    "\n",
    "# Initialize variables\n",
    "x = np.zeros(n)\n",
    "g = grad(x)\n",
    "d = -g\n",
    "Q = np.identity(n)\n",
    "\n",
    "# Define convergence criteria\n",
    "max_iter = 50\n",
    "tol = 1e-4\n",
    "\n",
    "# Conjugate gradient method\n",
    "for i in range(max_iter):\n",
    "    # Compute step size\n",
    "    alpha = - (g @ d) / (d @ Q @ d)\n",
    "    \n",
    "    # Update variables\n",
    "    x += alpha * d\n",
    "    g_new = grad(x)\n",
    "    beta = (g_new @ Q @ d) / (d @ Q @ d)\n",
    "    d = -g_new + beta * d\n",
    "    \n",
    "    # Check convergence\n",
    "    if np.linalg.norm(g_new) < tol:\n",
    "        break\n",
    "    \n",
    "    # Update Q-conjugate direction\n",
    "    Q = np.outer(g_new - g, g_new - g) / ((g_new - g) @ d)\n",
    "    \n",
    "    # Update gradient and Q-conjugate direction\n",
    "    g = g_new\n",
    "\n",
    "print(f'Conjugate gradient method converged after {i+1} iterations.')\n",
    "print(f'Solution: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42c7cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 300) (300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6.25654550e+00, -1.63578742e+01,  2.23384267e+01, -1.33333963e+01,\n",
       "        4.66859631e+00,  3.71251480e+01, -5.13854392e+00, -1.53668733e+01,\n",
       "        3.05822712e+01, -1.73971241e+01,  1.67722964e+00,  3.24043611e+01,\n",
       "        3.23127108e+00,  1.00880814e+01,  1.99166678e+01,  1.07877427e+01,\n",
       "       -8.84261311e+00,  1.06828367e+01,  6.12447518e+00,  1.32961388e+01,\n",
       "        1.17303095e+01, -1.56116540e+01,  2.41474114e+00,  9.32750987e+00,\n",
       "        8.32318304e+00,  1.72941776e+01,  3.50086440e+01,  4.37325337e+00,\n",
       "       -6.01576442e-01, -8.61231595e-01,  2.89138090e+01,  1.80601577e+01,\n",
       "       -1.77295692e+01, -1.51277300e+01, -3.21630510e+01,  3.08828147e+00,\n",
       "       -8.08160890e+00, -2.55488176e+01,  3.37426211e+01,  1.33535174e+00,\n",
       "       -4.84694102e+00,  9.22606733e+00,  3.78299263e+01, -3.04910671e+01,\n",
       "       -2.67939774e+01, -2.93646670e+00,  2.55205362e+01,  1.05379738e+01,\n",
       "        2.37902364e+01, -1.68231683e+01,  1.55350353e+01,  1.85637933e+00,\n",
       "       -2.38924015e+01,  1.41391192e+01, -3.00720165e+01, -1.28036794e+01,\n",
       "        3.70055077e+00, -7.72603707e+00,  3.01878532e+01, -1.28027171e+01,\n",
       "       -6.55354132e+00, -1.24453272e+01, -4.34355801e+00,  3.02712460e+01,\n",
       "        2.76023632e+00, -1.22000820e+01,  5.81277933e+00, -9.05452219e+00,\n",
       "       -2.24182019e+01, -1.43352358e+01,  2.62046708e+01, -1.09837720e+01,\n",
       "       -1.12650179e+00, -9.58398400e+00, -2.72916540e+00, -3.22378398e+00,\n",
       "        2.52440820e+01, -3.76961780e-02, -1.55292014e+00, -1.22120814e+01,\n",
       "        1.72429498e+01, -1.38928964e+00,  1.29637384e+01, -3.04882802e+01,\n",
       "        1.66283765e+01, -1.35018267e+01,  2.33972562e+01,  1.77124622e+01,\n",
       "       -2.37921864e+01, -6.32130930e+00,  2.22998424e+00,  1.57508941e+01,\n",
       "       -1.15560276e+01,  3.28845854e+01, -6.29725364e+00, -2.43908851e+01,\n",
       "       -2.27366583e+01,  5.79755987e+00,  2.84675229e+00, -3.65240236e+00,\n",
       "        2.93671548e+00,  1.74064745e-01, -2.07450398e+00,  3.08039282e+01,\n",
       "       -1.42032832e+01, -1.51049865e+01,  6.06900478e+00,  4.55415344e+01,\n",
       "       -1.80334820e+01, -1.68657051e+00, -9.76238159e+00, -7.17342919e+00,\n",
       "       -1.91280753e+00,  5.41545309e+00,  2.31492294e+01,  2.30170945e+01,\n",
       "        1.69050816e+01,  1.81353218e+00,  2.20017029e+01, -5.97257434e+00,\n",
       "       -1.56501260e+01, -1.19424210e+01,  1.67526809e+01,  2.70478146e+00,\n",
       "       -1.13269463e+00,  1.02778392e+01, -9.18017394e+00,  9.26705752e+00,\n",
       "        2.77133860e+01,  9.42031765e+00, -2.19026514e+01,  1.54793626e+01,\n",
       "        3.26768610e+00,  4.27279202e+01, -4.06276306e+00, -2.04540856e+01,\n",
       "       -3.37442846e+01,  1.23291490e+01, -1.72477240e+01,  9.37700128e+00,\n",
       "       -9.59551502e+00, -1.84534850e+00,  4.06361376e+00, -1.92074781e+01,\n",
       "        1.02033431e+01, -1.87823971e+01,  2.93427743e+01, -3.31177429e+00,\n",
       "       -1.47035939e+01,  7.24873024e+00, -4.14482871e+00, -8.30627240e+00,\n",
       "       -5.12851587e+00,  6.77220245e+00,  4.47206575e+00, -1.24146385e+01,\n",
       "       -1.35067546e+01,  1.10856115e+01, -1.68798510e+01, -9.02577593e+00,\n",
       "        8.75746865e-01, -1.16998985e+01, -5.55855705e+00,  1.47141149e+01,\n",
       "       -1.39162960e+01, -3.28812440e+01,  4.31990959e+01,  7.43729072e+00,\n",
       "        2.44680610e+00, -2.29406980e+00,  4.02890071e+00, -1.60041160e+01,\n",
       "        2.40809710e+01, -3.29979693e+01, -1.09646709e+01,  5.67894113e-01,\n",
       "       -1.62583525e+01,  1.65903294e+01, -3.83494818e+00, -2.16046192e+01,\n",
       "       -3.21512841e-01, -1.19859308e+01, -1.14753559e+01,  6.02908885e+00,\n",
       "        9.50031280e+00, -3.15445699e+00,  2.69846454e+01,  1.59128644e+01,\n",
       "       -1.52424182e+01, -8.19393105e+00, -1.27663527e+00, -1.24552294e+01,\n",
       "       -2.08534679e+01, -1.55201712e+01,  1.64309924e+01, -1.13701368e+01,\n",
       "        3.14216456e+00, -2.29415358e+01, -1.92910498e+01,  3.07713416e+01])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(A.T.shape, b.shape)\n",
    "np.dot(A.T,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d6d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 999 objective value fn: 42.62592724858708\n",
      "Solution: [-1.38221004e-02 -2.36302258e-03 -6.18699036e-02 -9.02559612e-02\n",
      " -6.35989641e-02 -7.42470928e-02  8.50202806e-03 -1.56836103e-01\n",
      "  2.99939626e-02  7.34381251e-02 -8.46978027e-02 -2.36152525e-01\n",
      "  1.36797813e-01 -1.28719597e-02 -1.22709707e-02  6.46785708e-02\n",
      "  5.87587974e-02 -5.95103076e-02 -5.85929318e-02  1.93430567e-02\n",
      " -1.23898553e-01  1.07175457e-01 -6.53479838e-02 -6.71634809e-02\n",
      "  1.10892231e-01 -2.19215421e-02  7.00449308e-02  1.88427962e-03\n",
      " -2.09601138e-01 -2.04252613e-01 -4.88124257e-02  1.20635224e-02\n",
      "  1.88864569e-01  1.03953431e-01  9.21686991e-02  1.66504679e-01\n",
      " -1.89502863e-02 -9.25343968e-02  7.74434730e-02  3.85660962e-02\n",
      "  4.15925497e-02  1.41844572e-01  1.75917076e-01 -2.85370037e-02\n",
      " -4.47119821e-02  2.97572992e-02  2.38300539e-03  5.22983234e-02\n",
      "  6.44299270e-02  3.97635960e-02 -8.60523313e-02  2.27237885e-02\n",
      " -1.98185838e-01 -4.09368238e-02  8.28453345e-03  1.05030299e-01\n",
      " -2.07981277e-01  1.64982742e-01 -1.29897493e-01 -1.31792290e-01\n",
      " -1.76389565e-01  5.49051942e-02  4.61537918e-02 -9.44277097e-02\n",
      "  1.56450190e-02  5.88079325e-03  1.62594015e-01  1.75801458e-02\n",
      " -9.68002128e-02 -1.14416052e-01 -2.56023456e-02 -4.42442347e-02\n",
      "  1.29315865e-01  1.09599889e-01  1.11001486e-03  1.00918224e-01\n",
      " -1.03192586e-01  1.36706016e-01  1.93404183e-01 -6.62892183e-02\n",
      "  9.59215282e-02  9.25545775e-02 -3.38018237e-02 -2.20452509e-01\n",
      "  1.43800644e-01  1.03291142e-01 -8.12992644e-02 -5.24805149e-02\n",
      "  2.34132094e-02  1.66578872e-01  8.27800387e-02  1.56069126e-01\n",
      " -5.27111672e-02 -5.52799280e-02  3.45623406e-02  8.72535017e-02\n",
      " -1.53369090e-02  3.67666776e-02 -3.44980143e-02 -7.96377279e-02\n",
      " -8.45788357e-02 -7.17583586e-02  1.25895154e-01  1.25717512e-01\n",
      "  3.46491574e-02 -1.49415937e-01 -3.42850626e-02  1.41469003e-01\n",
      "  6.89940442e-02  3.47299902e-02 -4.89759521e-02  4.86341473e-02\n",
      " -9.13451441e-02 -3.84774907e-02  2.46872691e-01  7.30305932e-03\n",
      " -6.56606785e-03  1.03073739e-01 -1.46725979e-01 -6.32387777e-02\n",
      "  7.84349117e-02 -3.86636242e-02 -3.62931885e-02 -1.10853820e-01\n",
      "  4.45355966e-02  8.78941859e-02  1.97911862e-02 -1.38023660e-01\n",
      "  1.02197636e-01 -4.88871740e-02  1.00510564e-01 -6.08346528e-02\n",
      " -9.89905199e-02 -5.04540154e-02  1.22973186e-01 -3.16919476e-02\n",
      " -2.62342901e-02 -1.00235899e-01 -1.71491468e-01  1.62192813e-02\n",
      "  1.46938086e-01  3.61402507e-02  1.42884090e-01 -4.16112859e-04\n",
      " -2.88416378e-01  6.80694956e-02 -8.58094525e-02  4.97355261e-02\n",
      " -6.92250120e-02 -1.25306328e-02 -5.29131017e-03 -1.98489221e-02\n",
      " -1.21035695e-01  8.59764641e-02  1.20535413e-01  1.13657891e-02\n",
      " -8.02542837e-02 -1.41839368e-02 -7.35255279e-02  6.56696656e-03\n",
      " -1.68571366e-01 -5.95727929e-02  8.37089368e-02  7.92603951e-03\n",
      " -4.49120713e-03  9.78730455e-02  1.17932382e-01 -1.51746924e-02\n",
      "  3.70462812e-02 -1.00563866e-01 -1.10471523e-02 -2.59354935e-02\n",
      "  9.60956654e-02  8.01239835e-02 -4.76968904e-02 -9.10568614e-02\n",
      "  8.31160378e-02 -1.78130601e-02  5.15930874e-02 -1.72921076e-02\n",
      "  4.57676949e-02  5.69009448e-02  8.12934090e-03  2.26584729e-01\n",
      "  4.83742943e-02 -5.16493550e-03 -9.55671369e-03 -5.81042270e-02\n",
      "  6.75208200e-03  9.12753467e-03 -1.32511363e-01  1.21964450e-01\n",
      "  3.17845838e-02  5.10830040e-02 -2.06795759e-05 -4.24191671e-02\n",
      "  6.58983735e-02 -2.29668182e-03  4.48630628e-02 -3.21324353e-02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backtracking_line_search(x, p, obj, grad_hess, alpha=0.25, beta=0.5):\n",
    "    # Initialize step size\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # Evaluate function and gradient at starting point\n",
    "    fx = obj(x)\n",
    "    gradfx, hessfx = grad_hess(x)\n",
    "    \n",
    "    # Evaluate function and gradient at next point\n",
    "    x_next = x + alpha*p\n",
    "    fx_next = obj(x_next)\n",
    "    \n",
    "    # Update step size until Armijo condition is satisfied\n",
    "    while fx_next > fx + alpha*alpha*np.dot(gradfx.T, p)/2:\n",
    "        alpha = beta*alpha\n",
    "        x_next = x + alpha*p\n",
    "        fx_next = obj(x_next)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "# Define problem size\n",
    "m = 300\n",
    "n = 200\n",
    "\n",
    "# Randomly generate A and b using a Gaussian random variable generator\n",
    "A = np.random.normal(size=(m, n))\n",
    "b = np.random.normal(size=m)\n",
    "\n",
    "# Define initial point and convergence tolerance\n",
    "x0 = np.zeros(n)\n",
    "tol = 1e-6\n",
    "\n",
    "def grad(x):\n",
    "    return A.T @ (A @ x - b)\n",
    "\n",
    "def grad_hess(x):\n",
    "    grad = A.T @ (A @ x - b)\n",
    "    hess = A.T @ A\n",
    "    return grad, hess\n",
    "\n",
    "def objective(x):\n",
    "    return 0.5 * np.linalg.norm(A @ x - b)**2\n",
    "\n",
    "x = np.zeros(A.shape[1])\n",
    "\n",
    "# fixed alpha\n",
    "for i in range(1000):\n",
    "    grad_f = grad(x)\n",
    "    d = -grad_f\n",
    "    alpha = backtracking_line_search(x, )\n",
    "    _x_ = x + alpha * d\n",
    "    if np.linalg.norm(_x_ - x) < 1e-10:\n",
    "        break\n",
    "    x = _x_\n",
    "    \n",
    "print(f'iterations: {i} objective value fn: {objective(x)}')\n",
    "print(f'Solution: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backtracking_line_search(x, p, obj, grad_hess, alpha=0.25, beta=0.5):\n",
    "    \"\"\" \n",
    "    Backtracking line search algorithm.\n",
    "    Inputs:\n",
    "        x: starting point\n",
    "        p: search direction\n",
    "        obj: objective function\n",
    "        grad_hess: function that returns the gradient and Hessian\n",
    "        alpha, beta: algorithm parameters\n",
    "    Returns:\n",
    "        alpha: step size\n",
    "    \"\"\"\n",
    "    # Initialize step size\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # Evaluate function and gradient at starting point\n",
    "    fx = obj(x)\n",
    "    gradfx, hessfx = grad_hess(x)\n",
    "    \n",
    "    # Evaluate function and gradient at next point\n",
    "    x_next = x + alpha*p\n",
    "    fx_next = obj(x_next)\n",
    "    \n",
    "    # Update step size until Armijo condition is satisfied\n",
    "    while fx_next > fx + alpha*alpha*np.dot(gradfx.T, p)/2:\n",
    "        alpha = beta*alpha\n",
    "        x_next = x + alpha*p\n",
    "        fx_next = obj(x_next)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "\n",
    "# Define problem size\n",
    "m = 300\n",
    "n = 200\n",
    "\n",
    "# Randomly generate A and b using a Gaussian random variable generator\n",
    "A = np.random.normal(size=(m, n))\n",
    "b = np.random.normal(size=m)\n",
    "\n",
    "# Define initial point and convergence tolerance\n",
    "x0 = np.zeros(n)\n",
    "tol = 1e-6\n",
    "\n",
    "# Define function to compute gradient and Hessian\n",
    "def grad_hess(x):\n",
    "    grad = A.T @ (A @ x - b)\n",
    "    hess = A.T @ A\n",
    "    return grad, hess\n",
    "\n",
    "# Define function to compute objective value\n",
    "def obj(x):\n",
    "    return 0.5 * np.linalg.norm(A @ x - b)**2\n",
    "\n",
    "# Conjugate gradient method\n",
    "x = x0\n",
    "r, _ = grad_hess(x)\n",
    "p = -r\n",
    "k = 0\n",
    "while np.linalg.norm(r) > tol:\n",
    "    alpha = backtracking_line_search(x, p, obj, grad_hess)\n",
    "    x = x + alpha * p\n",
    "    r_new, _ = grad_hess(x)\n",
    "    beta = max(0, np.linalg.norm(r_new)**2 / np.linalg.norm(r)**2)\n",
    "    p = -r_new + beta * p\n",
    "    r = r_new\n",
    "    k += 1\n",
    "    print(f\"Iteration {k}: objective value = {obj(x)}\")\n",
    "\n",
    "print(f\"Conjugate gradient method converged in {k} iterations.\")\n",
    "print(f\"Solution: x = {x}\")\n",
    "print(f\"Objective value at solution: {obj(x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0998848",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 300 is different from 200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Fixed alpha\u001b[39;00m\n\u001b[1;32m     72\u001b[0m x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n)\n\u001b[0;32m---> 73\u001b[0m x, num_iter, final_resid, errors_fixed \u001b[38;5;241m=\u001b[39m \u001b[43mconjugate_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfixed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixed alpha: iterations=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, objective value=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobjective(x, A, b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Backtracking line search\u001b[39;00m\n",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36mconjugate_gradient\u001b[0;34m(A, b, x, alpha_method, max_iter, tol)\u001b[0m\n\u001b[1;32m     42\u001b[0m errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m---> 45\u001b[0m     Ap \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m alpha_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfixed\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 300 is different from 200)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random matrix A and vector b\n",
    "m = 300\n",
    "n = 200\n",
    "A = np.random.normal(size=(m, n))\n",
    "b = np.random.normal(size=(m,))\n",
    "\n",
    "def objective(x, A, b):\n",
    "    \"\"\"\n",
    "    The objective function to minimize.\n",
    "\n",
    "    Parameters:\n",
    "        x (ndarray): The variable vector.\n",
    "        A (ndarray): The coefficient matrix of the system.\n",
    "        b (ndarray): The right-hand side of the system.\n",
    "\n",
    "    Returns:\n",
    "        float: The value of the objective function.\n",
    "    \"\"\"\n",
    "    return 0.5 * np.linalg.norm(A @ x - b)**2\n",
    "\n",
    "def grad(x, A, b):\n",
    "    \"\"\"\n",
    "    The gradient of the objective function.\n",
    "\n",
    "    Parameters:\n",
    "        x (ndarray): The variable vector.\n",
    "        A (ndarray): The coefficient matrix of the system.\n",
    "        b (ndarray): The right-hand side of the system.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The gradient of the objective function.\n",
    "    \"\"\"\n",
    "    return A.T @ (A @ x - b)\n",
    "\n",
    "def conjugate_gradient(A, b, x, alpha_method='fixed', max_iter=1000, tol=1e-10):\n",
    "    r = b - A @ x\n",
    "    p = r\n",
    "    rsold = r.T @ r\n",
    "    errors = []\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        Ap = A @ p.reshape(-1, 1)\n",
    "        alpha = None\n",
    "        \n",
    "        if alpha_method == 'fixed':\n",
    "            alpha = 1 / np.linalg.eigvalsh(A.T @ A)[-1]\n",
    "        elif alpha_method == 'bt':\n",
    "            alpha = rsold / (p.T @ Ap)\n",
    "            bt = (r.T @ Ap) / (p.T @ Ap)\n",
    "            p = r - bt * p\n",
    "        elif alpha_method == 'ls':\n",
    "            alpha = (r.T @ r) / (p.T @ Ap).flatten()\n",
    "        \n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap.flatten()\n",
    "        rsnew = r.T @ r\n",
    "        errors.append(np.linalg.norm(A @ x - b) / np.linalg.norm(b))\n",
    "        \n",
    "        if np.sqrt(rsnew) < tol:\n",
    "            break\n",
    "        \n",
    "        beta = rsnew / rsold\n",
    "        p = r + beta * p\n",
    "        rsold = rsnew\n",
    "    \n",
    "    return x, i+1, np.sqrt(rsnew), errors\n",
    "\n",
    "# Fixed alpha\n",
    "x0 = np.zeros(n)\n",
    "x, num_iter, final_resid, errors_fixed = conjugate_gradient(A, b, x0, alpha_method='fixed')\n",
    "print(f'Fixed alpha: iterations={num_iter}, objective value={objective(x, A, b)}')\n",
    "\n",
    "# Backtracking line search\n",
    "x0 = np.zeros(n)\n",
    "x, num_iter, final_resid, errors_bt = conjugate_gradient(A, b, x0, alpha_method='bt')\n",
    "print(f'Backtracking line search: iterations={num_iter}, objective value={objective(x, A, b)}')\n",
    "\n",
    "# Exact line search\n",
    "x0 = np.zeros(n)\n",
    "x, num_iter, final_resid, errors_ls = conjugate_gradient(A, b, x0, alpha_method='ls')\n",
    "print(f'Exact line search: iterations={num_iter}, objective value={objective(x, A, b)}')\n",
    "\n",
    "# Plot relative errors\n",
    "plt.semilogy(errors_fixed, label='Fixed alpha')\n",
    "plt.semilogy(errors_bt, label='Backtracking line search')\n",
    "plt.semilogy(errors_ls, label='Exact line search')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Relative error')\n",
    "plt.legend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a6c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
